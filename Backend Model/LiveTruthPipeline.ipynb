{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "This block installs the essential Python libraries for the project:\n",
    "\n",
    "1. **Transformers**: For leveraging pre-trained transformer models.\n",
    "2. **Accelerate**: For optimizing and accelerating model training.\n",
    "3. **PEFT**: For parameter-efficient fine-tuning.\n",
    "4. **TRL**: For training language models with reinforcement learning.\n",
    "5. **BitsAndBytes**: For memory-efficient model quantization.\n",
    "6. **Wandb**: For experiment tracking and logging.\n",
    "7. **Requests**: For making HTTP requests.\n",
    "8. **BeautifulSoup4**: For web scraping.\n",
    "9. **GoogleSearch-Python**: For programmatic Google searches.\n",
    "10. **NLTK**: For natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:32:24.852088Z",
     "iopub.status.busy": "2024-12-28T13:32:24.851496Z",
     "iopub.status.idle": "2024-12-28T13:34:18.227615Z",
     "shell.execute_reply": "2024-12-28T13:34:18.226580Z",
     "shell.execute_reply.started": "2024-12-28T13:32:24.852039Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install -U transformers \n",
    "%pip install -U accelerate \n",
    "%pip install -U peft \n",
    "%pip install -U trl \n",
    "%pip install -U bitsandbytes \n",
    "%pip install -U wandb\n",
    "%pip install requests\n",
    "%pip install beautifulsoup4\n",
    "%pip install googlesearch-python\n",
    "%pip install nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Environment and Loading the Model\n",
    "\n",
    "#### 1. **Importing Libraries**\n",
    "Key libraries like Transformers, HuggingFace Hub, and Torch are imported for model loading and configuration.\n",
    "\n",
    "#### 2. **Authentication**\n",
    "The HuggingFace token (`HUGGINGFACE_TOKEN`) is retrieved from environment variables for downloading models. Ensure the token is set.\n",
    "\n",
    "#### 3. **Model Setup**\n",
    "- **Base Model**: `NousResearch/Llama-2-7b-chat-hf`, a chat-optimized LLaMA-2 variant.\n",
    "- **Fine-Tuned Model**: Fine tuned model stored at `llama-fine-tuned1/pytorch/default/1`.\n",
    "\n",
    "### 4. **Torch Configuration**\n",
    "Configures `torch_dtype` and attention implementation based on GPU capabilities, with support for hardware acceleration via `flash-attn`.\n",
    "\n",
    "### 5. **QLoRA Setup**\n",
    "The `BitsAndBytesConfig` enables efficient 4-bit quantization with optimizations like double quantization for reduced memory usage.\n",
    "\n",
    "### 6. **Model & Tokenizer Loading**\n",
    "- The base model is loaded with QLoRA and device mapping.\n",
    "- The tokenizer is configured with EOS and padding tokens for chat-based tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:34:18.229739Z",
     "iopub.status.busy": "2024-12-28T13:34:18.229445Z",
     "iopub.status.idle": "2024-12-28T13:34:34.575466Z",
     "shell.execute_reply": "2024-12-28T13:34:34.574666Z",
     "shell.execute_reply.started": "2024-12-28T13:34:18.229711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:34:34.577024Z",
     "iopub.status.busy": "2024-12-28T13:34:34.576521Z",
     "iopub.status.idle": "2024-12-28T13:34:35.455426Z",
     "shell.execute_reply": "2024-12-28T13:34:35.454472Z",
     "shell.execute_reply.started": "2024-12-28T13:34:34.576989Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Get the token from environment variables\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "else:\n",
    "    print(\"HuggingFace token not found. Please set the HUGGINGFACE_TOKEN environment variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:34:53.849659Z",
     "iopub.status.busy": "2024-12-28T13:34:53.849320Z",
     "iopub.status.idle": "2024-12-28T13:34:53.854010Z",
     "shell.execute_reply": "2024-12-28T13:34:53.853181Z",
     "shell.execute_reply.started": "2024-12-28T13:34:53.849632Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model from Hugging Face hub\n",
    "base_model = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "#fine-tuned model\n",
    "fine_tuned_model = \"llama-fine-tuned1/pytorch/default/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:35:15.656909Z",
     "iopub.status.busy": "2024-12-28T13:35:15.656080Z",
     "iopub.status.idle": "2024-12-28T13:35:15.750593Z",
     "shell.execute_reply": "2024-12-28T13:35:15.749641Z",
     "shell.execute_reply.started": "2024-12-28T13:35:15.656863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# Set torch dtype and attention implementation\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install -qqq flash-attn\n",
    "    torch_dtype = torch.bfloat16\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:35:19.896764Z",
     "iopub.status.busy": "2024-12-28T13:35:19.896432Z",
     "iopub.status.idle": "2024-12-28T13:37:17.633467Z",
     "shell.execute_reply": "2024-12-28T13:37:17.632491Z",
     "shell.execute_reply.started": "2024-12-28T13:35:19.896736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    "    \n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Core Functions\n",
    "\n",
    "#### **Imports and Setup**\n",
    "Key libraries (`Transformers`, `PEFT`, `NLTK`, `BeautifulSoup`, `pandas`) enable:\n",
    "- Model initialization\n",
    "- Text analysis\n",
    "- Web scraping\n",
    "\n",
    "#### **Model Initialization**\n",
    "\n",
    "##### `initialise_base_model(base_model_dir)`\n",
    "- Loads a 4-bit quantized LLaMA model using `BitsAndBytesConfig`.\n",
    "- Configures for CUDA or CPU, loads tokenizer.\n",
    "- **Returns:** tokenizer, model, and device.\n",
    "\n",
    "##### `initialise_fine_tuned_model(base_model, adapter_dir)`\n",
    "- Loads a fine-tuned model via PEFT adapter.\n",
    "- Sets model to evaluation mode.\n",
    "- **Returns:** fine-tuned model.\n",
    "\n",
    "\n",
    "#### **Phase 1: News Analysis**\n",
    "\n",
    "##### `analyze_news(headline, tokenizer, model, device)`\n",
    "- **Input:** News headline.\n",
    "- **Process:** Generates:\n",
    "  - Confidence score (0â€“100)\n",
    "  - Detailed explanation about truthfulness.\n",
    "- **Output:** Model's response.\n",
    "\n",
    "##### `extract_confidence_score(response)`\n",
    "- **Input:** Generated response.\n",
    "- **Process:** Extracts confidence score using regex.\n",
    "- **Validation:** Ensures score is between 0 and 100.\n",
    "- **Output:** Extracted score or `None`.\n",
    "\n",
    "\n",
    "#### **Phase 2: Web Scraping and Summarization**\n",
    "\n",
    "##### `scrape_important_content(url)`\n",
    "- **Input:** URL.\n",
    "- **Process:** \n",
    "  - Fetches webpage using `BeautifulSoup`.\n",
    "  - Extracts headings (`h1`, `h2`, `h3`) and up to 8 key paragraphs.\n",
    "- **Output:** Scraped content or error message.\n",
    "\n",
    "##### `process_query(query, filename)`\n",
    "- **Input:** Search query and output filename.\n",
    "- **Process:** \n",
    "  - Extracts keywords from the query (removes stopwords).\n",
    "  - Performs Google search and retrieves URLs.\n",
    "  - Scrapes content from URLs and saves it to a CSV file.\n",
    "- **Output:** Scraped data.\n",
    "\n",
    "##### `generate_summary_with_llama(file_path, tokenizer, model, device)`\n",
    "- **Input:** File path to scraped data CSV.\n",
    "- **Process:** \n",
    "  - Reads and combines content from the CSV.\n",
    "  - Generates a concise summary (up to 100 words) using the LLaMA model.\n",
    "- **Output:** Generated summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T15:51:56.306963Z",
     "iopub.status.busy": "2024-12-27T15:51:56.306559Z",
     "iopub.status.idle": "2024-12-27T15:51:57.162828Z",
     "shell.execute_reply": "2024-12-27T15:51:57.161852Z",
     "shell.execute_reply.started": "2024-12-27T15:51:56.306933Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from transformers import BitsAndBytesConfig\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from googlesearch import search\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "# NLTK setup\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def initialise_base_model(base_model_dir):\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            llm_int8_enable_fp32_cpu_offload=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    # Load model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_dir,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_dir)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    base_model = base_model.to(device)\n",
    "    base_model.eval()\n",
    "\n",
    "    return tokenizer, base_model, device\n",
    "\n",
    "def initialise_fine_tuned_model(base_model, adapter_dir):\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_dir)\n",
    "        \n",
    "    # Move Model to Appropriate Device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# PHASE 1 FUNCTIONS\n",
    "def analyze_news(headline, tokenizer, model, device):\n",
    "    # Input Prompt\n",
    "    input_text = (\n",
    "        \"You are a news analyzer. Given the headline, provide a confidence score (0-100) indicating how likely the news is true, \"\n",
    "        \"and give a detailed explanation for your assessment. \"\n",
    "        f\"Headline: '{headline}'\\n\"\n",
    "    )\n",
    "\n",
    "    # Tokenize Input\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    # Generate Response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=250,   # Limits generated tokens only\n",
    "            num_beams=5,          # Enhance quality with beam search\n",
    "            temperature=0.7,      # Balance randomness\n",
    "            top_k=40,             # Limit to top-k tokens\n",
    "            top_p=0.9,            # Nucleus sampling\n",
    "            repetition_penalty=1.2  # Reduce repetitive outputs\n",
    "        )\n",
    "    \n",
    "    # Decode and Post-process\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "def extract_confidence_score(response):\n",
    "    match = re.search(r\"\\bconfidence(?:\\s+score(?:\\s(?:of|is)?)?)?\\s*[:=]?\\s*(\\d{1,3})\", response, re.IGNORECASE)\n",
    "    if match:\n",
    "        try:\n",
    "            score = int(match.group(1))\n",
    "            if 0 <= score <= 100:  # Ensure the score is within the valid range\n",
    "                return score\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None  # Return None if no valid score is found\n",
    "\n",
    "\n",
    "\n",
    "# PHASE 2 FUNCTIONS\n",
    "def scrape_important_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            return \"Failed to fetch content\"\n",
    "        \n",
    "        # Parse the webpage content\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Extract the main headings and paragraphs\n",
    "        headings = soup.find_all(['h1', 'h2', 'h3'])  # Extract headings\n",
    "        paragraphs = soup.find_all('p')  # Extract paragraphs\n",
    "        \n",
    "        # Combine content\n",
    "        content = \"\"\n",
    "        for h in headings:\n",
    "            content += h.get_text(strip=True) + \" | \"\n",
    "        for p in paragraphs[:8]:  # Limit paragraphs to avoid too much text\n",
    "            content += p.get_text(strip=True) + \" \"\n",
    "        \n",
    "        return content.strip() if content else \"No significant content found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "        \n",
    "def process_query(query, filename):\n",
    "    # Step 1: Extract keywords from the query\n",
    "    words = word_tokenize(query)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    keywords = [word for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "    # Step 2: Perform Google search using extracted keywords\n",
    "    search_query = \" \".join(keywords)\n",
    "    search_results = [url for url in search(search_query, num_results=10)]\n",
    "\n",
    "    # Step 3: Scrape content from search results\n",
    "    scraped_data = []\n",
    "    for url in search_results:\n",
    "        content = scrape_important_content(url)\n",
    "        scraped_data.append([url, content])\n",
    "\n",
    "    # Step 4: Save scraped data to a CSV file\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"URL\", \"Important Content\"])\n",
    "        writer.writerows(scraped_data)\n",
    "    return scraped_data\n",
    "\n",
    "def generate_summary_with_llama(file_path,tokenizer,model,device):\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    col = df['Important Content'].tolist()\n",
    "    corpus = [i for i in col if i not in (\"No significant content found.\",\"Failed to fetch content\")]\n",
    "    \n",
    "    combined_corpus = \"\\n\".join(corpus)[:4000]  # Limit the input to avoid exceeding model input size\n",
    "\n",
    "    input_text = (\n",
    "        f\"You are a news summarization expert. analyse the data scrapped from web which is: [{combined_corpus}] and provide an overall summary in maximum 100 words.\\n\"\n",
    "    )\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,   # Limit generated tokens\n",
    "            num_beams=5,          # Enhance quality with beam search\n",
    "            temperature=0.7,      # Balance randomness\n",
    "            top_k=40,             # Limit to top-k tokens\n",
    "            top_p=0.9,            # Nucleus sampling\n",
    "            repetition_penalty=1.2  # Reduce repetitive outputs\n",
    "        )\n",
    "\n",
    "    # Decode and post-process\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Pipeline\n",
    "- Integrates **Phase 1 (News Analysis)** and **Phase 2 (Web Scraping and Summarization)** into a unified process.\n",
    "- Output: Detailed response, confidence score, and summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T18:56:51.666225Z",
     "iopub.status.busy": "2024-12-19T18:56:51.665563Z",
     "iopub.status.idle": "2024-12-19T18:56:51.672262Z",
     "shell.execute_reply": "2024-12-19T18:56:51.671331Z",
     "shell.execute_reply.started": "2024-12-19T18:56:51.666190Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def combinedPipeline(txt):\n",
    "    adapter_dir = \"/kaggle/input/llama-fine-tuned1/pytorch/default/1\"\n",
    "    base_model_dir = \"NousResearch/Llama-2-7b-chat-hf\" \n",
    "    # Initialize Model\n",
    "    tokenizer, base_model, device = initialise_base_model(base_model_dir)\n",
    "    fine_tuned_model = initialise_fine_tuned_model(base_model, adapter_dir)\n",
    "    \n",
    "    # PHASE 1\n",
    "    headline = txt\n",
    "    fine_tune_response = analyze_news(headline, tokenizer, fine_tuned_model, device)\n",
    "    print(fine_tune_response)\n",
    "    \n",
    "    # Extract Confidence Score\n",
    "    confidence_score_phase1 = extract_confidence_score(fine_tune_response)\n",
    "    print(\"\\nExtracted Confidence Score:\")\n",
    "    print(confidence_score_phase1)\n",
    "    \n",
    "    #phase 2\n",
    "    filename = \"web_content_summary.csv\"\n",
    "    print(\"scrapping web\")\n",
    "    scraped_data = process_query(txt, filename)   \n",
    "\n",
    "    print(\"\\nReading scraped content from CSV and Generating summary using Llama model...\")\n",
    "    filepath = filename\n",
    "    news_summary = generate_summary_with_llama(filepath,tokenizer,base_model,device)\n",
    "    start_index = news_summary.find(\"provide an overall summary in maximum 100 words.\"\n",
    "    )\n",
    "    if start_index != -1:\n",
    "        news_summary = news_summary[start_index:]\n",
    "    print(news_summary)\n",
    "        \n",
    "    return fine_tune_response, confidence_score_phase1, news_summary    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T18:57:09.806022Z",
     "iopub.status.busy": "2024-12-19T18:57:09.805683Z",
     "iopub.status.idle": "2024-12-19T19:01:26.960475Z",
     "shell.execute_reply": "2024-12-19T19:01:26.959550Z",
     "shell.execute_reply.started": "2024-12-19T18:57:09.805992Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7741ff28eed447b09fd8cadcc5e9fa2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a news analyzer. Given the headline, provide a confidence score (0-100) indicating how likely the news is true, and give a detailed explanation for your assessment. Headline: 'Post Office Recruitment 2020: Big vacancy of over 1371 posts for 10th pass'\n",
      "Confidence Score: 80\n",
      "Explanation: The Post Office Recruitment 2020: Big vacancy of over 1371 posts for 10th pass is a false news article. The article claims that there are 1371 vacancies for 10th pass candidates in the post office recruitment 2020. However, there is no such information available on the official website of the India Post. In fact, the official website of the India Post does not mention anything about 1371 vacancies for 10th pass candidates in the post office recruitment 2020. Therefore, this news article is false and should be disregarded.\n",
      "The news article is created to mislead people who are looking for job opportunities in the post office. It is a common tactic used by fake news creators to spread false information and mislead people. In this case, the news article is false and has been created to mislead people who are looking for job opportunities in the post office.\n",
      "The confidence score for this news article is 80 as there is no information available on\n",
      "\n",
      "Extracted Confidence Score:\n",
      "80\n",
      "scrapping web\n",
      "\n",
      "Reading scraped content from CSV and Generating summary using Llama model...\n",
      "provide an overall summary in maximum 100 words.\n",
      "India Post Recruitment 2025 - Notification, Vacancies, Posts, Eligibility Criteria | India Post 2025 Recruitment | India Post Recruitment 2025 - Overview | India Post Recruitment 2025 - Important Dates | India Post Recruitment 2025 - Important Dates | India Post Recruitment 2025 - Eligibility Criteria | India Post Recruitment 2025 - Eligibility Criteria | India Post Recruitment 2025 - Syllabus | India Post 2025 Recruitment - Syllabus | India Post Recruitment 2025 - Exam Pattern | India Post Recruitment 2025 - Exam Pattern | India Post Recruitment 2025 - Admit Card | India Post Recruitment 2025 - Vacancies | India Post 2025 Recruitment -Vacancy | India Post Recruitment 2025 - Salary | India Post 2025 Recruitment - Salary | Frequently Asked Questions (FAQs) | Articles | Upcoming Competition Exams | Certifications By Top Providers | Explore Top Universities Across Globe | Related E-books & Sample Papers | Applications for Admissions  are open. | Stay up-to date withNews | Explore on Careers360 | Top Exams | Predictors & Ebooks | Exams by Category | Upcoming Events | The India Post Recruitment board hires staff for various positions in the Indian Postal Services. The various roles hired in the India Post Recruitment 2025 are important in running the communication department of the country. The India Post 2025 Recruitment conducts exams regularly every year for various Posts. Some of the important Posts that will be released by the India Post Recruitment 2025 are Gramin Dak Sevaks (GDS), Branch Post Masters (BPM), Multi Tasking Staff, Stenographers Grade-I, Assistant Branch Post Masters (ABPM), etc. The India Post notification for these Posts will be released on their official website, indiapost.gov\n",
      "@@@@@@@@pipeline working@@@@@@@@@@\n"
     ]
    }
   ],
   "source": [
    "txt='Post Office Recruitment 2020: Big vacancy of over 1371 posts for 10th pass'\n",
    "fine_tune_response, confidence_score_phase1, news_summary = combinedPipeline(txt)\n",
    "print(\"@@@@@@@@pipeline working@@@@@@@@@@\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Score Generation\n",
    "\n",
    "- **Purpose**: Analyzes the truthfulness of a news headline based on a fine-tuned LLaMA model's response and web-scraped content, returning a confidence score.\n",
    "- **Input**: Takes the fine-tuned model's response, web-scraped news summary, and headline as input.\n",
    "- **Output**: Returns the confidence score and overall result on whether the news is true or false.\n",
    "- **Example Usage** on headline = `Post Office Recruitment 2020: Big vacancy of over 1371 posts for 10th pass`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T15:52:11.356213Z",
     "iopub.status.busy": "2024-12-27T15:52:11.354990Z",
     "iopub.status.idle": "2024-12-27T15:54:53.521172Z",
     "shell.execute_reply": "2024-12-27T15:54:53.520147Z",
     "shell.execute_reply.started": "2024-12-27T15:52:11.356161Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54259032b9a48388aede076b7e994aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You are a news analyser. under the result from a fine tuned LLM which is [The Post Office Recruitment 2020: Big vacancy of over 1371 posts for 10th pass is a false news article. The article claims that there are 1371 vacancies for 10th pass candidates in the post office recruitment 2020. However, there is no such information available on the official website of the India Post. In fact, the official website of the India Post does not mention anything about 1371 vacancies for 10th pass candidates in the post office recruitment 2020. Therefore, this news article is false and should be disregarded.The news article is created to mislead people who are looking for job opportunities in the post office. It is a common tactic used by fake news creators to spread false information and mislead people. In this case, the news article is false and has been created to mislead people who are looking for job opportunities in the post office.The confidence score for this news article is 80 as there is no information available on] and the data scrapped from web which is: [India Post Recruitment 2025 - Notification, Vacancies, Posts, Eligibility Criteria | India Post 2025 Recruitment | India Post Recruitment 2025 - Overview | India Post Recruitment 2025 - Important Dates | India Post Recruitment 2025 - Important Dates | India Post Recruitment 2025 - Eligibility Criteria | India Post Recruitment 2025 - Eligibility Criteria | India Post Recruitment 2025 - Syllabus | India Post 2025 Recruitment - Syllabus | India Post Recruitment 2025 - Exam Pattern | India Post Recruitment 2025 - Exam Pattern | India Post Recruitment 2025 - Admit Card | India Post Recruitment 2025 - Vacancies | India Post 2025 Recruitment -Vacancy | India Post Recruitment 2025 - Salary | India Post 2025 Recruitment - Salary | Frequently Asked Questions (FAQs) | Articles | Upcoming Competition Exams | Certifications By Top Providers | Explore Top Universities Across Globe | Related E-books & Sample Papers | Applications for Admissions  are open. | Stay up-to date withNews | Explore on Careers360 | Top Exams | Predictors & Ebooks | Exams by Category | Upcoming Events | The India Post Recruitment board hires staff for various positions in the Indian Postal Services. The various roles hired in the India Post Recruitment 2025 are important in running the communication department of the country. The India Post 2025 Recruitment conducts exams regularly every year for various Posts. Some of the important Posts that will be released by the India Post Recruitment 2025 are Gramin Dak Sevaks (GDS), Branch Post Masters (BPM), Multi Tasking Staff, Stenographers Grade-I, Assistant Branch Post Masters (ABPM), etc. The India Post notification for these Posts will be released on their official website, indiapost.gov] and provide an overall resultt that whether the news is true and false and a confidence score to it for the headline [Post Office Recruitment 2020: Big vacancy of over 1371 posts for 10th pass].\\n\\nUnder the result from a fine-tuned LLM, the news article [Post Office Recruitment 2020: Big vacancy of over 1371 posts for 10th pass] is found to be false with a confidence score of 80. The article claims that there are 1371 vacancies for 10th pass candidates in the post office recruitment 2020, but there is no such information available on the official website of the India Post. In fact, the official website of the India Post does not mention anything about 1371 vacancies for 10th pass candidates in the post office recruitment 2020. Therefore, this news article is false and should be disregarded.\\n\\nThe data scraped from the web shows that the India Post Recruitment board hires staff for various positions in the Indian Postal Services. The various roles hired in the India Post Recruitment 2025 are important in running the communication department of the country. The India Post 2025 Recruitment conducts exams regularly every year for various Posts. Some of the important Posts that will be released by the India Post Recruitment 2025 are Gramin Dak Sevaks (GDS), Branch Post Masters (BPM), Multi Tasking Staff, Stenographers Grade-I, Assistant Branch Post Masters (ABPM), etc. The India Post notification for these Posts will be released on their official website, indiapost.gov.in.\\n\\nIn conclusion, the news article [Post Office Recruitment 2020: Big vacancy of over 1371 posts for 10th pass] is false and should be disregarded. The India Post Recruitment 2025 does not mention anything about 1371 vacancies for 10th pass candidates in the post office recruitment 2020. The data scraped from the web shows that the India Post Recruitment board hires staff for various positions in the Indian Postal Services, and the various Posts that will be released by the India Post Recruitment 2025 are important in running the communication department of the country.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confidence score\n",
    "def generatescore(tokenizer,model,device,fine_tune_response,news_summary,headline):\n",
    "# fine_tune_response, confidence_score_phase1, news_summary\n",
    "    input_text = (\n",
    "        f\"You are a news analyser. under the result from a fine tuned LLM which is [{fine_tune_response}] and the data scrapped from web which is: [{news_summary}] and provide an overall resultt that whether the news is true and false and a confidence score to it for the headline [{headline}].\\n\"\n",
    "    )\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,   # Limit generated tokens\n",
    "            num_beams=5,          # Enhance quality with beam search\n",
    "            temperature=0.7,      # Balance randomness\n",
    "            top_k=40,             # Limit to top-k tokens\n",
    "            top_p=0.9,            # Nucleus sampling\n",
    "            repetition_penalty=1.2  # Reduce repetitive outputs\n",
    "        )\n",
    "\n",
    "    # Decode and post-process\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "# adapter_dir = \"/kaggle/input/llama-fine-tuned1/pytorch/default/1\"\n",
    "base_model_dir = \"NousResearch/Llama-2-7b-chat-hf\" \n",
    "# Initialize Model\n",
    "tokenizer, base_model, device = initialise_base_model(base_model_dir)\n",
    "\n",
    "headline = 'Post Office Recruitment 2020: Big vacancy of over 1371 posts for 10th pass'\n",
    "fine_tune_response, confidence_score_phase1, news_summary = combinedPipeline(txt)\n",
    "\n",
    "generatescore(tokenizer,base_model,device,fine_tune_response,news_summary,headline)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 735,
     "modelInstanceId": 3092,
     "sourceId": 4297,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 195286,
     "modelInstanceId": 172948,
     "sourceId": 202715,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
