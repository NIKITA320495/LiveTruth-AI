{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "This code block installs the necessary Python libraries to work with HuggingFace Transformers, datasets, and tools for fine-tuning large language models. \n",
    "\n",
    "### Libraries Overview:\n",
    "- **Transformers**: Provides APIs and tools for loading pre-trained models and fine-tuning them on custom datasets.\n",
    "- **Datasets**: Simplifies dataset handling for machine learning tasks.\n",
    "- **Accelerate**: Optimizes and speeds up distributed model training.\n",
    "- **PEFT (Parameter-Efficient Fine-Tuning)**: Allows fine-tuning large models efficiently.\n",
    "- **TRL (Transformer Reinforcement Learning)**: Tools for RL with Transformers.\n",
    "- **Bitsandbytes**: Enables 8-bit model optimization for faster training and inference.\n",
    "- **Weights & Biases (wandb)**: A tool for experiment tracking and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T16:01:10.258359Z",
     "iopub.status.busy": "2024-12-12T16:01:10.257974Z",
     "iopub.status.idle": "2024-12-12T16:02:46.841570Z",
     "shell.execute_reply": "2024-12-12T16:02:46.840307Z",
     "shell.execute_reply.started": "2024-12-12T16:01:10.258329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U transformers \n",
    "%pip install -U datasets \n",
    "%pip install -U accelerate \n",
    "%pip install -U peft \n",
    "%pip install -U trl \n",
    "%pip install -U bitsandbytes \n",
    "%pip install -U wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up and Preparing for Model Fine-Tuning\n",
    "\n",
    "#### Importing Required Libraries\n",
    "First we import all necessary libraries for model loading, fine-tuning, and training. This includes HuggingFace Transformers, PEFT (Parameter-Efficient Fine-Tuning), and other utility libraries like `wandb`, `torch`, and `datasets`.\n",
    "#### Authenticating with HuggingFace Hub\n",
    "Then we log into HuggingFace Hub using a token retrieved from environment variables for downloading our base model (`NousResearch/Llama-2-7b-chat-hf`). Ensure the `HUGGINGFACE_TOKEN` environment variable is set with your personal HuggingFace access token.\n",
    "#### Configuring and Loading the Model\n",
    "The next block sets up QLoRA (Quantized Low-Rank Adaptation) for efficient fine-tuning using `BitsAndBytesConfig`. It loads the base model with 4-bit quantization and the tokenizer for text generation tasks.\n",
    "#### Defining LoRA Fine-Tuning Parameters\n",
    "Finally, we define the parameters for PEFT using the LoraConfig class. These parameters include `lora_alpha`, `lora_dropout`, `r`, and the `task type` (CAUSAL_LM). These settings optimize the fine-tuning process for causal language modeling tasks by efficiently adapting the pre-trained model for the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T16:02:46.843406Z",
     "iopub.status.busy": "2024-12-12T16:02:46.843058Z",
     "iopub.status.idle": "2024-12-12T16:03:04.720681Z",
     "shell.execute_reply": "2024-12-12T16:03:04.719991Z",
     "shell.execute_reply.started": "2024-12-12T16:02:46.843374Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch, wandb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, setup_chat_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T12:05:33.407401Z",
     "iopub.status.busy": "2024-12-12T12:05:33.406374Z",
     "iopub.status.idle": "2024-12-12T12:05:33.714491Z",
     "shell.execute_reply": "2024-12-12T12:05:33.713563Z",
     "shell.execute_reply.started": "2024-12-12T12:05:33.407357Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Get the token from environment variables\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "else:\n",
    "    print(\"HuggingFace token not found. Please set the HUGGINGFACE_TOKEN environment variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model from Hugging Face hub\n",
    "base_model = \"NousResearch/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CODE TO CHECK FOR GPU FOR RUNNIGNG THE CODE\n",
    "\n",
    "# Set torch dtype and attention implementation\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install -qqq flash-attn\n",
    "    torch_dtype = torch.bfloat16\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\"\n",
    "\n",
    "\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation \n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T07:51:41.159763Z",
     "iopub.status.busy": "2024-12-09T07:51:41.158871Z",
     "iopub.status.idle": "2024-12-09T07:51:41.164613Z",
     "shell.execute_reply": "2024-12-09T07:51:41.163941Z",
     "shell.execute_reply.started": "2024-12-09T07:51:41.159724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "peft_params = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data for Fine-Tuning\n",
    "\n",
    "This section prepares the dataset for fine-tuning a chat-based AI model:\n",
    "\n",
    "1. **Data Cleaning**:\n",
    "   - Loaded the dataset (`IFND.csv`) and standardized the `Label` column (`TRUE` → 1, `FALSE` → 0).\n",
    "   \n",
    "2. **Balancing**:\n",
    "   - Sampled equal numbers of `TRUE` and `FALSE` labels for training (1000 each) and validation (200 each).\n",
    "\n",
    "3. **Formatting**:\n",
    "   - Converted the data into a Hugging Face Dataset.\n",
    "   - Defined a chat-based template with system instructions and user queries for classification.\n",
    "\n",
    "4. **Final Output**:\n",
    "   - Processed datasets are ready for tokenization and fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T07:51:53.277776Z",
     "iopub.status.busy": "2024-12-09T07:51:53.277427Z",
     "iopub.status.idle": "2024-12-09T07:51:58.458187Z",
     "shell.execute_reply": "2024-12-09T07:51:58.457315Z",
     "shell.execute_reply.started": "2024-12-09T07:51:53.277745Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/3420174063.py:4: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('/kaggle/input/infd-dataset-final/IFND.csv', encoding='ISO-8859-1')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statement</th>\n",
       "      <th>Category</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WHO praises India's Aarogya Setu app, says it ...</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In Delhi, Deputy US Secretary of State Stephen...</td>\n",
       "      <td>VIOLENCE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LAC tensions: China's strategy behind delibera...</td>\n",
       "      <td>TERROR</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India has signed 250 documents on Space cooper...</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tamil Nadu chief minister's mother passes away...</td>\n",
       "      <td>ELECTION</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Statement  Category Label\n",
       "0  WHO praises India's Aarogya Setu app, says it ...  COVID-19  True\n",
       "1  In Delhi, Deputy US Secretary of State Stephen...  VIOLENCE  True\n",
       "2  LAC tensions: China's strategy behind delibera...    TERROR  True\n",
       "3  India has signed 250 documents on Space cooper...  COVID-19  True\n",
       "4  Tamil Nadu chief minister's mother passes away...  ELECTION  True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'ISO-8859-1' with the detected encoding\n",
    "df = pd.read_csv('IFND.csv', encoding='ISO-8859-1')\n",
    "df = df.iloc[:, :3]\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T07:51:58.459863Z",
     "iopub.status.busy": "2024-12-09T07:51:58.459595Z",
     "iopub.status.idle": "2024-12-09T07:51:58.500109Z",
     "shell.execute_reply": "2024-12-09T07:51:58.499315Z",
     "shell.execute_reply.started": "2024-12-09T07:51:58.459836Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['Label'] = df['Label'].astype(str)\n",
    "# Replace \"true\" with \"TRUE\" in the 'label' column\n",
    "df['Label'] = df['Label'].replace({'True': 'TRUE', 'False': 'FALSE','Fake':'FALSE'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T07:51:58.501233Z",
     "iopub.status.busy": "2024-12-09T07:51:58.500990Z",
     "iopub.status.idle": "2024-12-09T07:51:59.266567Z",
     "shell.execute_reply": "2024-12-09T07:51:59.265806Z",
     "shell.execute_reply.started": "2024-12-09T07:51:58.501209Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['Statement'] = df['Statement'].astype(str)\n",
    "df['Label'] = df['Label'].map({'TRUE': 1, 'FALSE': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T07:51:59.268360Z",
     "iopub.status.busy": "2024-12-09T07:51:59.268091Z",
     "iopub.status.idle": "2024-12-09T07:52:01.067318Z",
     "shell.execute_reply": "2024-12-09T07:52:01.066580Z",
     "shell.execute_reply.started": "2024-12-09T07:51:59.268333Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26680c013e7426e9c1ddb2cdf6b8323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Sample true and false labels\n",
    "true_rows = df[df['Label'] == 1].sample(1000)\n",
    "false_rows = df[df['Label'] == 0].sample(1000)\n",
    "\n",
    "# Combine the two datasets\n",
    "subset_df = pd.concat([true_rows, false_rows])\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(subset_df, split=\"train\")\n",
    "\n",
    "# Define instruction\n",
    "instruction = \"\"\"You are an AI assistant trained to classify news articles based on their content.\n",
    "Your task is to analyze the content of a given news article and determine if the article is factually TRUE or FALSE.\n",
    "Provide accurate and well-reasoned classifications.\"\"\"\n",
    "\n",
    "# Define the formatting function\n",
    "def format_chat_template(row):\n",
    "    row_json = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": f\"Please classify the following news article: {row['Statement']}\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"The news article is classified as: {row['Label']}\"}\n",
    "    ]\n",
    "    # Flatten the row into a format suitable for tokenization\n",
    "    formatted_input = \"\\n\".join([entry[\"content\"] for entry in row_json])\n",
    "    # Tokenize the input manually\n",
    "    encoding = tokenizer(formatted_input, truncation=True, padding='max_length', max_length=512)\n",
    "    return encoding\n",
    "\n",
    "# Apply the formatting function to the dataset\n",
    "formatted_dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc=4,  # Use multiple processors for speed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T07:52:12.036356Z",
     "iopub.status.busy": "2024-12-09T07:52:12.035997Z",
     "iopub.status.idle": "2024-12-09T07:52:12.043192Z",
     "shell.execute_reply": "2024-12-09T07:52:12.042330Z",
     "shell.execute_reply.started": "2024-12-09T07:52:12.036319Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Statement', 'Category', 'Label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T07:52:13.058107Z",
     "iopub.status.busy": "2024-12-09T07:52:13.057751Z",
     "iopub.status.idle": "2024-12-09T07:52:13.819889Z",
     "shell.execute_reply": "2024-12-09T07:52:13.819116Z",
     "shell.execute_reply.started": "2024-12-09T07:52:13.058076Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a4c050017b4efe810e3ca3b3942f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Statement', 'Category', 'Label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 400\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Example: Load dataset into DataFrame\n",
    "\n",
    "# Sample true and false labels\n",
    "true_rows = df[df['Label'] == 1].sample(200)\n",
    "false_rows = df[df['Label'] == 0].sample(200)\n",
    "\n",
    "# Combine the two datasets\n",
    "subset_df = pd.concat([true_rows, false_rows])\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(subset_df, split=\"train\")\n",
    "\n",
    "# Load tokenizer (ensure you're using a model that supports chat functionality)\n",
    "\n",
    "# Define instruction\n",
    "instruction = \"\"\"You are an AI assistant trained to classify news articles based on their content.\n",
    "Your task is to analyze the content of a given news article and determine if the article is factually TRUE or FALSE.\n",
    "Provide accurate and well-reasoned classifications.\"\"\"\n",
    "\n",
    "# Define the formatting function\n",
    "def format_chat_template(row):\n",
    "    row_json = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": f\"Please classify the following news article: {row['Statement']}\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"The news article is classified as: {row['Label']}\"}\n",
    "    ]\n",
    "    # Flatten the row into a format suitable for tokenization\n",
    "    formatted_input = \"\\n\".join([entry[\"content\"] for entry in row_json])\n",
    "    # Tokenize the input manually\n",
    "    encoding = tokenizer(formatted_input, truncation=True, padding='max_length', max_length=512)\n",
    "    return encoding\n",
    "\n",
    "# Apply the formatting function to the dataset\n",
    "formatted_val_dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc=4,  # Use multiple processors for speed\n",
    ")\n",
    "\n",
    "# Save formatted dataset to a JSONL file for fine-tuning\n",
    "formatted_val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning and Saving the Model\n",
    "\n",
    "1. **Gradient Checkpointing**: Enabled gradient checkpointing for efficient memory usage during training.\n",
    "2. **Training Configuration**: Defined training parameters using `TrainingArguments`, including batch size, learning rate, gradient accumulation, and optimizer.\n",
    "3. **Training**: Used `SFTTrainer` to fine-tune the model with the processed dataset and PEFT configuration.\n",
    "4. **Saving Results**: Saved the fine-tuned model and tokenizer to the `./results` directory.\n",
    "5. **Download Link**: Provided a direct link to download the results as a ZIP file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T12:12:27.001489Z",
     "iopub.status.busy": "2024-12-09T12:12:27.000590Z",
     "iopub.status.idle": "2024-12-09T12:12:27.005613Z",
     "shell.execute_reply": "2024-12-09T12:12:27.004549Z",
     "shell.execute_reply.started": "2024-12-09T12:12:27.001451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "def checkpointed_model(*inputs):\n",
    "    return checkpoint.checkpoint(model, *inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T09:31:21.283300Z",
     "iopub.status.busy": "2024-12-09T09:31:21.282445Z",
     "iopub.status.idle": "2024-12-09T10:59:06.281208Z",
     "shell.execute_reply": "2024-12-09T10:59:06.280286Z",
     "shell.execute_reply.started": "2024-12-09T09:31:21.283260Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 1:27:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.947800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.245000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.270900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.073600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.161300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.955100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.136700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.007300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.180300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.249300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.966800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.029700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.929300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>1.133200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.973200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.979300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.935200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>1.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.939500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>1.016100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.914400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>1.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.127700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>1.097300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>1.088900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.985200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>1.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.976300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.795000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.845800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.876000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>1.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.950200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.018400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>1.180900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.918600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>0.883800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.064400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>1.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.992800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.818700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.025500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>1.215300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.961400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>0.998300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.308200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.930100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.826200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>0.936600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.062100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>1.099100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.823000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>1.105400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.157900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>1.117900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.043100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>1.272300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.887800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>1.109900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.855900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>1.043300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.949900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>0.968400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.060600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>0.992200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>0.991300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>0.853300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.204900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=1.0461262998580934, metrics={'train_runtime': 5261.7761, 'train_samples_per_second': 0.38, 'train_steps_per_second': 0.38, 'total_flos': 4.0801677606912e+16, 'train_loss': 1.0461262998580934, 'epoch': 1.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=500,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset,\n",
    "    peft_config=peft_params,\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T10:59:52.670341Z",
     "iopub.status.busy": "2024-12-09T10:59:52.669451Z",
     "iopub.status.idle": "2024-12-09T10:59:53.201149Z",
     "shell.execute_reply": "2024-12-09T10:59:53.200215Z",
     "shell.execute_reply.started": "2024-12-09T10:59:52.670294Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./results/tokenizer_config.json',\n",
       " './results/special_tokens_map.json',\n",
       " './results/tokenizer.model',\n",
       " './results/added_tokens.json',\n",
       " './results/tokenizer.json')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./results\")\n",
    "tokenizer.save_pretrained(\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T11:19:38.671363Z",
     "iopub.status.busy": "2024-12-09T11:19:38.670447Z",
     "iopub.status.idle": "2024-12-09T11:19:38.677949Z",
     "shell.execute_reply": "2024-12-09T11:19:38.677087Z",
     "shell.execute_reply.started": "2024-12-09T11:19:38.671325Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='results.zip' target='_blank'>results.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/results.zip"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'results.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the fine-tuned model is a random headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T16:56:05.423614Z",
     "iopub.status.busy": "2024-12-12T16:56:05.423236Z",
     "iopub.status.idle": "2024-12-12T16:57:15.597039Z",
     "shell.execute_reply": "2024-12-12T16:57:15.596327Z",
     "shell.execute_reply.started": "2024-12-12T16:56:05.423581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab5a811809d4c698da5be1db4a22480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      "You are a news analyzer. Given the headline, determine if it's true or false, and provide a detailed explanation. Also, classify the news into a theme like politics, sports, education, etc.\n",
      "Headline: 'was ramnath kovind ever been the president of india'\n",
      "The news is false.Ram Nath Kovind is the 14th President of India, serving since 2017. He was born on October 1, 1945, in Paralakhemundi, Odisha. He is a member of the Bharatiya Janata Party (BJP) and served as the Governor of Bihar from 2015 to 2017 before being elected as the President of India in 2017.\n",
      "The news is false.Ram Nath Kovind is the 14th President of India, serving since 2017. He was born on October 1, 1945, in Paralakhemundi, Odisha. He is a member of the Bharatiya Janata Party (BJP) and served as the Governor of Bihar from 2015 to 2017 before being elected as the President of India in 2017.\n",
      "The news is false.Ram Nath Kovind is the 14th President of India, serving since 2017.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Paths\n",
    "adapter_dir = \"/kaggle/input/llama-fine-tuned1/pytorch/default/1\"\n",
    "base_model_dir = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Quantization Config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_dir)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, adapter_dir)\n",
    "\n",
    "# Move Model to GPU/CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Input Prompt\n",
    "input_text = (\n",
    "    \"You are a news analyzer. Given the headline, determine if it's true or false, and provide a detailed explanation. \"\n",
    "    \"Also, classify the news into a theme like politics, sports, education, etc.\\n\"\n",
    "    \"Headline: 'was ramnath kovind ever been the president of india'\\n\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    ")\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# Generate Response\n",
    "# Generate Response\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=250,   # Limits generated tokens only\n",
    "        num_beams=5,          # Enhance quality with beam search\n",
    "        temperature=0.7,      # Balance randomness\n",
    "        top_k=40,             # Limit to top-k tokens\n",
    "        top_p=0.9,            # Nucleus sampling\n",
    "        repetition_penalty=1.2  # Reduce repetitive outputs\n",
    "    )\n",
    "\n",
    "# Decode and Post-process\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "response = response.strip()\n",
    "\n",
    "# Print Response\n",
    "print(\"Generated Response:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6256219,
     "sourceId": 10136878,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 735,
     "modelInstanceId": 3092,
     "sourceId": 4297,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 121027,
     "modelInstanceId": 100936,
     "sourceId": 120005,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 188524,
     "modelInstanceId": 166196,
     "sourceId": 194925,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
